{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network in PyTorch"
      ],
      "metadata": {
        "id": "Itqc_2bMZKWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "4ENAX5Y8XdKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Tcbhc427Llnv"
      },
      "outputs": [],
      "source": [
        "import torch                                  # Uses Entire PyTorch Library\n",
        "import torch.nn as nn                         # Imports PyTorch's Neural Network Toolkit\n",
        "import torch.nn.functional as functions       # Has common NN functions like ReLu and Sigmoid\n",
        "import torch.optim as optimizer               # Brings in Optimizers to update model params after backpropogation\n",
        "from torch.utils.data import DataLoader       # Helps load our train/test data into our model\n",
        "\n",
        "import numpy as np                            # Geenral math library we may need later on\n",
        "from torchvision import datasets, transforms  # Brings in example datasets and tools to format them\n",
        "from torchsummary import summary              # Tool to show the architecture of our network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Dataset and Hyperparameters"
      ],
      "metadata": {
        "id": "efOsGaICXfwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define training hyperparameters\n",
        "n_epochs = 5\n",
        "batch_size_train = 64\n",
        "batch_size_test = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Define your data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)\n"
      ],
      "metadata": {
        "id": "cn8r1_bfLp3g"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Neural Network Architecture"
      ],
      "metadata": {
        "id": "6JtfXcl3Xjat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = functions.relu(functions.max_pool2d(self.conv1(x), 2))\n",
        "        x = functions.relu(functions.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = functions.relu(self.fc1(x))\n",
        "        x = functions.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = functions.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lqOWHRXTLtyS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create network\n",
        "model = Net()\n",
        "\n",
        "# uncomment to print network summary\n",
        "summary(model, (1, 28, 28), device=\"cpu\")\n",
        "\n",
        "# define loss function and optimizer\n",
        "optimizer = optimizer.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnprn0C_LwQ3",
        "outputId": "c3ccbbf2-5f4b-4366-97fa-fcae5bda852c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 24, 24]             260\n",
            "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
            "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
            "            Linear-4                   [-1, 50]          16,050\n",
            "            Linear-5                   [-1, 10]             510\n",
            "================================================================\n",
            "Total params: 21,840\n",
            "Trainable params: 21,840\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.15\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Train Function"
      ],
      "metadata": {
        "id": "gdOO4OaXXnSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(model):\n",
        "  # Set the model in training mode\n",
        "  model.train()\n",
        "\n",
        "  # Train the model for some number of epochs\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "      # For every Image - Label pair in this current batch...\n",
        "      for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)               # Pass image data into the model\n",
        "          loss = loss_function(outputs, labels) # Compare prediction to actual answer\n",
        "          \n",
        "          # Backward pass and optimization\n",
        "          optimizer.zero_grad()                 # Clears previous backpropogation results.\n",
        "          loss.backward()                       # Performs Backpropogation\n",
        "          optimizer.step()                      # Applies backpropogation results\n",
        "          \n",
        "          # Print training status\n",
        "          if batch_idx % 100 == 0:\n",
        "              percent_complete = 100. * batch_idx / len(train_loader)\n",
        "              print(f\"Epoch: {epoch} [{batch_idx * len(inputs)}/{len(train_loader.dataset)} ({percent_complete:.0f}%)]\\tLoss: {loss.item():.6f}\")\n"
      ],
      "metadata": {
        "id": "xjnq73RbMCAR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Test Function"
      ],
      "metadata": {
        "id": "fqMc4Ev9XqXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testModel(model):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy on test set: %d %%' % (100 * correct / total))\n"
      ],
      "metadata": {
        "id": "QMz6lK3tXsVU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Training and Testing"
      ],
      "metadata": {
        "id": "v7S3eKMBYG51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainModel(model)\n",
        "testModel(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Sb4a00YLRx",
        "outputId": "fb09e2ab-c138-4ec9-c14f-e9d0d1d89511"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 [0/60000 (0%)]\tLoss: 2.303477\n",
            "Epoch: 0 [6400/60000 (11%)]\tLoss: 0.881523\n",
            "Epoch: 0 [12800/60000 (21%)]\tLoss: 1.029139\n",
            "Epoch: 0 [19200/60000 (32%)]\tLoss: 0.445954\n",
            "Epoch: 0 [25600/60000 (43%)]\tLoss: 0.273671\n",
            "Epoch: 0 [32000/60000 (53%)]\tLoss: 0.258224\n",
            "Epoch: 0 [38400/60000 (64%)]\tLoss: 0.345704\n",
            "Epoch: 0 [44800/60000 (75%)]\tLoss: 0.401346\n",
            "Epoch: 0 [51200/60000 (85%)]\tLoss: 0.372374\n",
            "Epoch: 0 [57600/60000 (96%)]\tLoss: 0.387496\n",
            "Epoch: 1 [0/60000 (0%)]\tLoss: 0.385967\n",
            "Epoch: 1 [6400/60000 (11%)]\tLoss: 0.221749\n",
            "Epoch: 1 [12800/60000 (21%)]\tLoss: 0.480937\n",
            "Epoch: 1 [19200/60000 (32%)]\tLoss: 0.250368\n",
            "Epoch: 1 [25600/60000 (43%)]\tLoss: 0.257325\n",
            "Epoch: 1 [32000/60000 (53%)]\tLoss: 0.443058\n",
            "Epoch: 1 [38400/60000 (64%)]\tLoss: 0.338358\n",
            "Epoch: 1 [44800/60000 (75%)]\tLoss: 0.319236\n",
            "Epoch: 1 [51200/60000 (85%)]\tLoss: 0.136328\n",
            "Epoch: 1 [57600/60000 (96%)]\tLoss: 0.299989\n",
            "Epoch: 2 [0/60000 (0%)]\tLoss: 0.208063\n",
            "Epoch: 2 [6400/60000 (11%)]\tLoss: 0.425756\n",
            "Epoch: 2 [12800/60000 (21%)]\tLoss: 0.232054\n",
            "Epoch: 2 [19200/60000 (32%)]\tLoss: 0.188494\n",
            "Epoch: 2 [25600/60000 (43%)]\tLoss: 0.213291\n",
            "Epoch: 2 [32000/60000 (53%)]\tLoss: 0.380949\n",
            "Epoch: 2 [38400/60000 (64%)]\tLoss: 0.180662\n",
            "Epoch: 2 [44800/60000 (75%)]\tLoss: 0.153677\n",
            "Epoch: 2 [51200/60000 (85%)]\tLoss: 0.294307\n",
            "Epoch: 2 [57600/60000 (96%)]\tLoss: 0.351675\n",
            "Epoch: 3 [0/60000 (0%)]\tLoss: 0.201492\n",
            "Epoch: 3 [6400/60000 (11%)]\tLoss: 0.210987\n",
            "Epoch: 3 [12800/60000 (21%)]\tLoss: 0.253704\n",
            "Epoch: 3 [19200/60000 (32%)]\tLoss: 0.163004\n",
            "Epoch: 3 [25600/60000 (43%)]\tLoss: 0.196962\n",
            "Epoch: 3 [32000/60000 (53%)]\tLoss: 0.481847\n",
            "Epoch: 3 [38400/60000 (64%)]\tLoss: 0.378737\n",
            "Epoch: 3 [44800/60000 (75%)]\tLoss: 0.402569\n",
            "Epoch: 3 [51200/60000 (85%)]\tLoss: 0.251076\n",
            "Epoch: 3 [57600/60000 (96%)]\tLoss: 0.384804\n",
            "Epoch: 4 [0/60000 (0%)]\tLoss: 0.199647\n",
            "Epoch: 4 [6400/60000 (11%)]\tLoss: 0.137595\n",
            "Epoch: 4 [12800/60000 (21%)]\tLoss: 0.172956\n",
            "Epoch: 4 [19200/60000 (32%)]\tLoss: 0.127268\n",
            "Epoch: 4 [25600/60000 (43%)]\tLoss: 0.126775\n",
            "Epoch: 4 [32000/60000 (53%)]\tLoss: 0.163760\n",
            "Epoch: 4 [38400/60000 (64%)]\tLoss: 0.180280\n",
            "Epoch: 4 [44800/60000 (75%)]\tLoss: 0.083865\n",
            "Epoch: 4 [51200/60000 (85%)]\tLoss: 0.190661\n",
            "Epoch: 4 [57600/60000 (96%)]\tLoss: 0.133948\n",
            "Accuracy on test set: 98 %\n"
          ]
        }
      ]
    }
  ]
}